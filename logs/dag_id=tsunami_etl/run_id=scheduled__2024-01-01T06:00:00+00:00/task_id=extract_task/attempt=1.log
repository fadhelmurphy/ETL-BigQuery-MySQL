[2024-02-27T13:47:54.417+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T13:47:54.419+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T13:47:54.420+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T13:47:54.425+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T13:47:54.430+0000] {standard_task_runner.py:60} INFO - Started process 539 to run task
[2024-02-27T13:47:54.433+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '25', '--raw', '--subdir', 'DAGS_FOLDER/bbc_news_etl.py', '--cfg-path', '/tmp/tmp98l6aev3']
[2024-02-27T13:47:54.435+0000] {standard_task_runner.py:88} INFO - Job 25: Subtask extract_task
[2024-02-27T13:47:54.463+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 169dadc36ff0
[2024-02-27T13:47:54.488+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T13:47:57.693+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: timed out
[2024-02-27T13:48:00.706+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: timed out
[2024-02-27T13:48:00.712+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:00.713+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T13:48:00.733+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/bbc_news_etl.py", line 44, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id='locator-173007', dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T13:48:00.757+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T134754, end_date=20240227T134800
[2024-02-27T13:48:00.774+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 25 for task extract_task (could not locate runnable browser; 539)
[2024-02-27T13:48:00.823+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T13:48:00.838+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T14:53:03.796+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T14:53:03.798+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T14:53:03.798+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T14:53:03.803+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T14:53:03.806+0000] {standard_task_runner.py:60} INFO - Started process 177 to run task
[2024-02-27T14:53:03.808+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/tsunami.py', '--cfg-path', '/tmp/tmpye2tucd8']
[2024-02-27T14:53:03.809+0000] {standard_task_runner.py:88} INFO - Job 9: Subtask extract_task
[2024-02-27T14:53:03.826+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host b5306f90dbdb
[2024-02-27T14:53:03.849+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T14:53:03.981+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:03.981+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:03.982+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:03.982+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T14:53:03.983+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami.py", line 42, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T14:53:03.986+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T145303, end_date=20240227T145303
[2024-02-27T14:53:03.990+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 9 for task extract_task (could not locate runnable browser; 177)
[2024-02-27T14:53:03.996+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T14:53:04.005+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:00:01.893+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:00:01.895+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:00:01.895+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:00:01.901+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:00:01.904+0000] {standard_task_runner.py:60} INFO - Started process 312 to run task
[2024-02-27T17:00:01.907+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp6373yirn']
[2024-02-27T17:00:01.909+0000] {standard_task_runner.py:88} INFO - Job 9: Subtask extract_task
[2024-02-27T17:00:01.930+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:00:02.005+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:00:02.009+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 46, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:00:02.023+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T170001, end_date=20240227T170002
[2024-02-27T17:00:02.074+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 9 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 312)
[2024-02-27T17:00:02.129+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:00:02.149+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:04:20.416+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:04:20.420+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:04:20.420+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:04:20.429+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:04:20.433+0000] {standard_task_runner.py:60} INFO - Started process 523 to run task
[2024-02-27T17:04:20.436+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpbez2wybf']
[2024-02-27T17:04:20.438+0000] {standard_task_runner.py:88} INFO - Job 24: Subtask extract_task
[2024-02-27T17:04:20.458+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:04:20.482+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:04:20.483+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:04:20.486+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T170420, end_date=20240227T170420
[2024-02-27T17:04:20.489+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 24 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 523)
[2024-02-27T17:04:20.494+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:04:20.504+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:06:25.454+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:06:25.456+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:06:25.456+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:06:25.461+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:06:25.465+0000] {standard_task_runner.py:60} INFO - Started process 715 to run task
[2024-02-27T17:06:25.467+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp6se1ve8e']
[2024-02-27T17:06:25.468+0000] {standard_task_runner.py:88} INFO - Job 40: Subtask extract_task
[2024-02-27T17:06:25.484+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:06:25.506+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:06:25.507+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/credentials.json'
[2024-02-27T17:06:25.509+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T170625, end_date=20240227T170625
[2024-02-27T17:06:25.514+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 40 for task extract_task ([Errno 2] No such file or directory: '../auth/json/credentials.json'; 715)
[2024-02-27T17:06:25.523+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:06:25.532+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:09:48.284+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:09:48.287+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:09:48.287+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:09:48.293+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:09:48.302+0000] {standard_task_runner.py:60} INFO - Started process 914 to run task
[2024-02-27T17:09:48.308+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpth6_r47p']
[2024-02-27T17:09:48.315+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask extract_task
[2024-02-27T17:09:48.401+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:09:48.475+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:09:48.477+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:09:48.480+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T170948, end_date=20240227T170948
[2024-02-27T17:09:48.485+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 8 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 914)
[2024-02-27T17:09:48.495+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:09:48.506+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:16:15.843+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:16:15.846+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:16:15.846+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:16:15.851+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:16:15.856+0000] {standard_task_runner.py:60} INFO - Started process 1249 to run task
[2024-02-27T17:16:15.858+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpx5annjcq']
[2024-02-27T17:16:15.859+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T17:16:15.877+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:16:15.902+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:16:15.903+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:16:15.905+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T171615, end_date=20240227T171615
[2024-02-27T17:16:15.909+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 1249)
[2024-02-27T17:16:15.957+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:16:15.963+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:20:05.971+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:20:05.974+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:20:05.974+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:20:05.979+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:20:05.984+0000] {standard_task_runner.py:60} INFO - Started process 1564 to run task
[2024-02-27T17:20:05.987+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpuiooo749']
[2024-02-27T17:20:05.989+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T17:20:06.009+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:20:06.033+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:20:06.034+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/credentials.json'
[2024-02-27T17:20:06.036+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T172005, end_date=20240227T172006
[2024-02-27T17:20:06.040+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: '/credentials.json'; 1564)
[2024-02-27T17:20:06.044+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:20:06.049+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:46:43.521+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:46:43.524+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:46:43.524+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:46:43.529+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:46:43.535+0000] {standard_task_runner.py:60} INFO - Started process 2002 to run task
[2024-02-27T17:46:43.538+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '23', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpa5upxwb2']
[2024-02-27T17:46:43.539+0000] {standard_task_runner.py:88} INFO - Job 23: Subtask extract_task
[2024-02-27T17:46:43.557+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:46:43.580+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:46:43.580+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T17:46:43.582+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T174643, end_date=20240227T174643
[2024-02-27T17:46:43.586+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 23 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 2002)
[2024-02-27T17:46:43.594+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:46:43.600+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:51:34.387+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:51:34.390+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T17:51:34.390+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:51:34.396+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T17:51:34.400+0000] {standard_task_runner.py:60} INFO - Started process 2286 to run task
[2024-02-27T17:51:34.402+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpykvdy0w4']
[2024-02-27T17:51:34.404+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T17:51:34.424+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:51:34.447+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T17:51:34.448+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'dags/credentials.json'
[2024-02-27T17:51:34.450+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T175134, end_date=20240227T175134
[2024-02-27T17:51:34.454+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: 'dags/credentials.json'; 2286)
[2024-02-27T17:51:34.460+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:51:34.466+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:09:34.844+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:09:34.847+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:09:34.847+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:09:34.858+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:09:34.865+0000] {standard_task_runner.py:60} INFO - Started process 300 to run task
[2024-02-27T18:09:34.867+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp0ulni9kv']
[2024-02-27T18:09:34.869+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:09:34.887+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host cf5ca63d0f74
[2024-02-27T18:09:34.909+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:09:34.910+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:09:34.912+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T180934, end_date=20240227T180934
[2024-02-27T18:09:34.916+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 300)
[2024-02-27T18:09:34.924+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:09:34.929+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:19:35.003+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:19:35.006+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:19:35.006+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:19:35.011+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:19:35.015+0000] {standard_task_runner.py:60} INFO - Started process 298 to run task
[2024-02-27T18:19:35.017+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp1owv99_x']
[2024-02-27T18:19:35.019+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:19:35.038+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 38ece35a2749
[2024-02-27T18:19:35.067+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:19:35.068+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(credentials_path))
  File "/usr/local/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/usr/local/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/local/lib/python3.8/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
[2024-02-27T18:19:35.071+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T181935, end_date=20240227T181935
[2024-02-27T18:19:35.075+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task (Expecting value: line 1 column 1 (char 0); 298)
[2024-02-27T18:19:35.120+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:19:35.126+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:23:04.602+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:23:04.605+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:23:04.605+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:23:04.610+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:23:04.612+0000] {standard_task_runner.py:60} INFO - Started process 300 to run task
[2024-02-27T18:23:04.614+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp9h3idkf7']
[2024-02-27T18:23:04.615+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:23:04.632+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:23:04.654+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:23:04.654+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(os.getenv('CREDENTIALS_KEY')))
  File "/usr/local/lib/python3.8/json/__init__.py", line 341, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not NoneType
[2024-02-27T18:23:04.657+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T182304, end_date=20240227T182304
[2024-02-27T18:23:04.660+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task (the JSON object must be str, bytes or bytearray, not NoneType; 300)
[2024-02-27T18:23:04.668+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:23:04.674+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:38:23.139+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:38:23.142+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:38:23.143+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:38:23.149+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:38:23.153+0000] {standard_task_runner.py:60} INFO - Started process 700 to run task
[2024-02-27T18:38:23.155+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpx2srch7f']
[2024-02-27T18:38:23.157+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:38:23.178+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:38:23.202+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:38:23.204+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:38:23.209+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T183823, end_date=20240227T183823
[2024-02-27T18:38:23.214+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 700)
[2024-02-27T18:38:23.251+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:38:23.258+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:39:31.342+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:39:31.345+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:39:31.345+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:39:31.350+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:39:31.354+0000] {standard_task_runner.py:60} INFO - Started process 1005 to run task
[2024-02-27T18:39:31.357+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpkvymp_65']
[2024-02-27T18:39:31.359+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:39:31.381+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:39:31.459+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:39:31.460+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:39:31.462+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T183931, end_date=20240227T183931
[2024-02-27T18:39:31.466+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 1005)
[2024-02-27T18:39:31.498+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:39:31.505+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:46:48.895+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:46:48.897+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:46:48.898+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:46:48.903+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:46:48.907+0000] {standard_task_runner.py:60} INFO - Started process 308 to run task
[2024-02-27T18:46:48.910+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpc16o0pri']
[2024-02-27T18:46:48.912+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:46:48.933+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:46:48.957+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:46:48.958+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:46:48.960+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T184648, end_date=20240227T184648
[2024-02-27T18:46:48.964+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 308)
[2024-02-27T18:46:49.007+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:46:49.015+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:49:56.128+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:49:56.131+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:49:56.131+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:49:56.136+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:49:56.139+0000] {standard_task_runner.py:60} INFO - Started process 858 to run task
[2024-02-27T18:49:56.141+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpghui_19v']
[2024-02-27T18:49:56.142+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:49:56.158+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:49:56.179+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:49:56.180+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/airflow/dags/credentials.json'
[2024-02-27T18:49:56.182+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T184956, end_date=20240227T184956
[2024-02-27T18:49:56.186+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: '/home/airflow/dags/credentials.json'; 858)
[2024-02-27T18:49:56.198+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:49:56.203+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:51:02.915+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:51:02.918+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:51:02.918+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:51:02.923+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:51:02.930+0000] {standard_task_runner.py:60} INFO - Started process 1156 to run task
[2024-02-27T18:51:02.933+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp5b3pxn3x']
[2024-02-27T18:51:02.935+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:51:02.956+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:51:02.980+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:51:02.982+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/airflow/dags/credentials.json'
[2024-02-27T18:51:02.989+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T185102, end_date=20240227T185102
[2024-02-27T18:51:02.994+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: '/home/airflow/dags/credentials.json'; 1156)
[2024-02-27T18:51:03.034+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:51:03.041+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:58:38.219+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:58:38.222+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [queued]>
[2024-02-27T18:58:38.222+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:58:38.227+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 06:00:00+00:00
[2024-02-27T18:58:38.230+0000] {standard_task_runner.py:60} INFO - Started process 447 to run task
[2024-02-27T18:58:38.233+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T06:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp2tnrhudi']
[2024-02-27T18:58:38.235+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T18:58:38.254+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T06:00:00+00:00 [running]> on host 37df473fe932
[2024-02-27T18:58:38.278+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T06:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T06:00:00+00:00'
[2024-02-27T18:58:38.476+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:38.477+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:38.477+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:38.478+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T18:58:38.479+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 45, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard',
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T18:58:38.482+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T060000, start_date=20240227T185838, end_date=20240227T185838
[2024-02-27T18:58:38.486+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task (could not locate runnable browser; 447)
[2024-02-27T18:58:38.508+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:58:38.514+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
