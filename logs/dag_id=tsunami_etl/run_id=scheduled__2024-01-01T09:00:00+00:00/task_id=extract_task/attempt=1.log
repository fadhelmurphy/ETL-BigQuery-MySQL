[2024-02-27T13:48:06.241+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T13:48:06.244+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T13:48:06.244+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T13:48:06.249+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T13:48:06.252+0000] {standard_task_runner.py:60} INFO - Started process 572 to run task
[2024-02-27T13:48:06.255+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/bbc_news_etl.py', '--cfg-path', '/tmp/tmp1kt85zpa']
[2024-02-27T13:48:06.256+0000] {standard_task_runner.py:88} INFO - Job 28: Subtask extract_task
[2024-02-27T13:48:06.274+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 169dadc36ff0
[2024-02-27T13:48:06.301+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T13:48:06.469+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:06.470+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:06.471+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:06.471+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T13:48:06.472+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/bbc_news_etl.py", line 44, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id='locator-173007', dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T13:48:06.475+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T134806, end_date=20240227T134806
[2024-02-27T13:48:06.481+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 28 for task extract_task (could not locate runnable browser; 572)
[2024-02-27T13:48:06.489+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T13:48:06.497+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T14:53:08.820+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T14:53:08.823+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T14:53:08.823+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T14:53:08.828+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T14:53:08.830+0000] {standard_task_runner.py:60} INFO - Started process 210 to run task
[2024-02-27T14:53:08.833+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/tsunami.py', '--cfg-path', '/tmp/tmp25mt64l5']
[2024-02-27T14:53:08.834+0000] {standard_task_runner.py:88} INFO - Job 12: Subtask extract_task
[2024-02-27T14:53:08.851+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host b5306f90dbdb
[2024-02-27T14:53:08.874+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T14:53:12.017+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: timed out
[2024-02-27T14:53:15.031+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: timed out
[2024-02-27T14:53:15.034+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:15.035+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T14:53:15.046+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami.py", line 42, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T14:53:15.064+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T145308, end_date=20240227T145315
[2024-02-27T14:53:15.080+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 12 for task extract_task (could not locate runnable browser; 210)
[2024-02-27T14:53:15.102+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T14:53:15.117+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:00:07.701+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:00:07.703+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:00:07.703+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:00:07.708+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T17:00:07.712+0000] {standard_task_runner.py:60} INFO - Started process 345 to run task
[2024-02-27T17:00:07.714+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp2dxb7c43']
[2024-02-27T17:00:07.715+0000] {standard_task_runner.py:88} INFO - Job 12: Subtask extract_task
[2024-02-27T17:00:07.732+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:00:07.754+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T17:00:07.754+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 46, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:00:07.760+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T170007, end_date=20240227T170007
[2024-02-27T17:00:07.765+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 12 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 345)
[2024-02-27T17:00:07.768+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:00:07.776+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:04:25.619+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:04:25.622+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:04:25.622+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:04:25.628+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T17:04:25.631+0000] {standard_task_runner.py:60} INFO - Started process 556 to run task
[2024-02-27T17:04:25.633+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpjn2afsj5']
[2024-02-27T17:04:25.635+0000] {standard_task_runner.py:88} INFO - Job 27: Subtask extract_task
[2024-02-27T17:04:25.655+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:04:25.680+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T17:04:25.681+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:04:25.683+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T170425, end_date=20240227T170425
[2024-02-27T17:04:25.687+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 27 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 556)
[2024-02-27T17:04:25.729+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:04:25.737+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:06:29.919+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:06:29.921+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:06:29.921+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:06:29.927+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T17:06:29.929+0000] {standard_task_runner.py:60} INFO - Started process 748 to run task
[2024-02-27T17:06:29.931+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '43', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp2mxl349x']
[2024-02-27T17:06:29.933+0000] {standard_task_runner.py:88} INFO - Job 43: Subtask extract_task
[2024-02-27T17:06:29.950+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:06:29.973+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T17:06:29.974+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/credentials.json'
[2024-02-27T17:06:29.976+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T170629, end_date=20240227T170629
[2024-02-27T17:06:29.981+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 43 for task extract_task ([Errno 2] No such file or directory: '../auth/json/credentials.json'; 748)
[2024-02-27T17:06:29.991+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:06:29.999+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:09:56.136+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:09:56.138+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:09:56.139+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:09:56.145+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T17:09:56.156+0000] {standard_task_runner.py:60} INFO - Started process 947 to run task
[2024-02-27T17:09:56.159+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpj6gufdf0']
[2024-02-27T17:09:56.161+0000] {standard_task_runner.py:88} INFO - Job 11: Subtask extract_task
[2024-02-27T17:09:56.183+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:09:56.217+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T17:09:56.219+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:09:56.223+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T170956, end_date=20240227T170956
[2024-02-27T17:09:56.227+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 11 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 947)
[2024-02-27T17:09:56.262+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:09:56.272+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:51:48.409+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:51:48.411+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T17:51:48.411+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:51:48.417+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T17:51:48.420+0000] {standard_task_runner.py:60} INFO - Started process 2385 to run task
[2024-02-27T17:51:48.423+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmppeq8z8ye']
[2024-02-27T17:51:48.424+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask extract_task
[2024-02-27T17:51:48.444+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:51:48.469+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T17:51:48.469+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'dags/credentials.json'
[2024-02-27T17:51:48.472+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T175148, end_date=20240227T175148
[2024-02-27T17:51:48.475+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 29 for task extract_task ([Errno 2] No such file or directory: 'dags/credentials.json'; 2385)
[2024-02-27T17:51:48.483+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:51:48.490+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:09:48.359+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:09:48.362+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:09:48.362+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:09:48.368+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T18:09:48.370+0000] {standard_task_runner.py:60} INFO - Started process 401 to run task
[2024-02-27T18:09:48.372+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp_yla6xsc']
[2024-02-27T18:09:48.374+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask extract_task
[2024-02-27T18:09:48.390+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host cf5ca63d0f74
[2024-02-27T18:09:48.413+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T18:09:48.414+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:09:48.416+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T180948, end_date=20240227T180948
[2024-02-27T18:09:48.420+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 29 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 401)
[2024-02-27T18:09:48.426+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:09:48.431+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:19:48.542+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:19:48.544+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:19:48.544+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:19:48.550+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T18:19:48.553+0000] {standard_task_runner.py:60} INFO - Started process 397 to run task
[2024-02-27T18:19:48.555+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpmoxwtlvl']
[2024-02-27T18:19:48.557+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask extract_task
[2024-02-27T18:19:48.575+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host 38ece35a2749
[2024-02-27T18:19:48.598+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T18:19:48.599+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(credentials_path))
  File "/usr/local/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/usr/local/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/local/lib/python3.8/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
[2024-02-27T18:19:48.601+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T181948, end_date=20240227T181948
[2024-02-27T18:19:48.605+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 29 for task extract_task (Expecting value: line 1 column 1 (char 0); 397)
[2024-02-27T18:19:48.613+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:19:48.619+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:23:20.395+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:23:20.398+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:23:20.398+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:23:20.403+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T18:23:20.406+0000] {standard_task_runner.py:60} INFO - Started process 399 to run task
[2024-02-27T18:23:20.408+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpimf73eny']
[2024-02-27T18:23:20.409+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask extract_task
[2024-02-27T18:23:20.425+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:23:20.447+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T18:23:20.447+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(os.getenv('CREDENTIALS_KEY')))
  File "/usr/local/lib/python3.8/json/__init__.py", line 341, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not NoneType
[2024-02-27T18:23:20.450+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T182320, end_date=20240227T182320
[2024-02-27T18:23:20.453+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 29 for task extract_task (the JSON object must be str, bytes or bytearray, not NoneType; 399)
[2024-02-27T18:23:20.465+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:23:20.471+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:39:47.554+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:39:47.557+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:39:47.557+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:39:47.562+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T18:39:47.565+0000] {standard_task_runner.py:60} INFO - Started process 1104 to run task
[2024-02-27T18:39:47.567+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpm00h1ncq']
[2024-02-27T18:39:47.569+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask extract_task
[2024-02-27T18:39:47.588+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:39:47.612+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T18:39:47.613+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:39:47.615+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T183947, end_date=20240227T183947
[2024-02-27T18:39:47.619+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 29 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 1104)
[2024-02-27T18:39:47.627+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:39:47.632+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:47:04.771+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:47:04.774+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [queued]>
[2024-02-27T18:47:04.774+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:47:04.780+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 09:00:00+00:00
[2024-02-27T18:47:04.784+0000] {standard_task_runner.py:60} INFO - Started process 407 to run task
[2024-02-27T18:47:04.786+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T09:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmptnni9q96']
[2024-02-27T18:47:04.788+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask extract_task
[2024-02-27T18:47:04.805+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T09:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:47:04.828+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T09:00:00+00:00'
[2024-02-27T18:47:04.829+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:47:04.831+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T090000, start_date=20240227T184704, end_date=20240227T184704
[2024-02-27T18:47:04.834+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 29 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 407)
[2024-02-27T18:47:04.846+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:47:04.852+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
