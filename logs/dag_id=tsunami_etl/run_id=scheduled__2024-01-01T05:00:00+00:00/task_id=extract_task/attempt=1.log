[2024-02-27T13:47:51.613+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T13:47:51.615+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T13:47:51.616+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T13:47:51.622+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T13:47:51.629+0000] {standard_task_runner.py:60} INFO - Started process 528 to run task
[2024-02-27T13:47:51.632+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/bbc_news_etl.py', '--cfg-path', '/tmp/tmp22jacgft']
[2024-02-27T13:47:51.634+0000] {standard_task_runner.py:88} INFO - Job 24: Subtask extract_task
[2024-02-27T13:47:51.663+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 169dadc36ff0
[2024-02-27T13:47:51.689+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T13:47:51.901+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:47:51.902+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:47:51.903+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:47:51.903+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T13:47:51.905+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/bbc_news_etl.py", line 44, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id='locator-173007', dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T13:47:51.914+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T134751, end_date=20240227T134751
[2024-02-27T13:47:51.922+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 24 for task extract_task (could not locate runnable browser; 528)
[2024-02-27T13:47:51.941+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T13:47:51.952+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T14:53:02.183+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T14:53:02.186+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T14:53:02.186+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T14:53:02.191+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T14:53:02.194+0000] {standard_task_runner.py:60} INFO - Started process 166 to run task
[2024-02-27T14:53:02.196+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/tsunami.py', '--cfg-path', '/tmp/tmp5qe65jbl']
[2024-02-27T14:53:02.197+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask extract_task
[2024-02-27T14:53:02.212+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host b5306f90dbdb
[2024-02-27T14:53:02.234+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T14:53:02.399+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:02.400+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:02.401+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:02.401+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T14:53:02.402+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami.py", line 42, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T14:53:02.405+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T145302, end_date=20240227T145302
[2024-02-27T14:53:02.410+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 8 for task extract_task (could not locate runnable browser; 166)
[2024-02-27T14:53:02.423+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T14:53:02.432+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:00:00.000+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:00:00.003+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:00:00.004+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:00:00.009+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:00:00.014+0000] {standard_task_runner.py:60} INFO - Started process 301 to run task
[2024-02-27T17:00:00.017+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpihweq3mr']
[2024-02-27T17:00:00.020+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask extract_task
[2024-02-27T17:00:00.081+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:00:00.136+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:00:00.136+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 46, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:00:00.139+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T170000, end_date=20240227T170000
[2024-02-27T17:00:00.144+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 8 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 301)
[2024-02-27T17:00:00.161+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:00:00.180+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:04:18.513+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:04:18.516+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:04:18.516+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:04:18.522+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:04:18.527+0000] {standard_task_runner.py:60} INFO - Started process 512 to run task
[2024-02-27T17:04:18.529+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '23', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpadsz321d']
[2024-02-27T17:04:18.531+0000] {standard_task_runner.py:88} INFO - Job 23: Subtask extract_task
[2024-02-27T17:04:18.552+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:04:18.585+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:04:18.586+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:04:18.590+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T170418, end_date=20240227T170418
[2024-02-27T17:04:18.595+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 23 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 512)
[2024-02-27T17:04:18.624+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:04:18.632+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:06:23.926+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:06:23.929+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:06:23.929+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:06:23.934+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:06:23.938+0000] {standard_task_runner.py:60} INFO - Started process 704 to run task
[2024-02-27T17:06:23.940+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '39', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpj2f3zdaz']
[2024-02-27T17:06:23.942+0000] {standard_task_runner.py:88} INFO - Job 39: Subtask extract_task
[2024-02-27T17:06:23.962+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:06:23.987+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:06:23.988+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/credentials.json'
[2024-02-27T17:06:23.993+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T170623, end_date=20240227T170623
[2024-02-27T17:06:23.997+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 39 for task extract_task ([Errno 2] No such file or directory: '../auth/json/credentials.json'; 704)
[2024-02-27T17:06:24.044+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:06:24.052+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:09:45.143+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:09:45.146+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:09:45.146+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:09:45.151+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:09:45.175+0000] {standard_task_runner.py:60} INFO - Started process 903 to run task
[2024-02-27T17:09:45.182+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp3_rd0l6_']
[2024-02-27T17:09:45.185+0000] {standard_task_runner.py:88} INFO - Job 7: Subtask extract_task
[2024-02-27T17:09:45.206+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:09:45.230+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:09:45.231+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:09:45.234+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T170945, end_date=20240227T170945
[2024-02-27T17:09:45.238+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 7 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 903)
[2024-02-27T17:09:45.285+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:09:45.293+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:16:11.064+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:16:11.067+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:16:11.067+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:16:11.073+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:16:11.081+0000] {standard_task_runner.py:60} INFO - Started process 1216 to run task
[2024-02-27T17:16:11.084+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpvv8c2awa']
[2024-02-27T17:16:11.086+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T17:16:11.107+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:16:11.143+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:16:11.144+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:16:11.146+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T171611, end_date=20240227T171611
[2024-02-27T17:16:11.150+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 1216)
[2024-02-27T17:16:11.184+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:16:11.194+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:20:00.900+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:20:00.903+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:20:00.904+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:20:00.914+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:20:00.928+0000] {standard_task_runner.py:60} INFO - Started process 1531 to run task
[2024-02-27T17:20:00.932+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpl8lacief']
[2024-02-27T17:20:00.934+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T17:20:00.965+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:20:01.010+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:20:01.011+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/credentials.json'
[2024-02-27T17:20:01.014+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T172000, end_date=20240227T172001
[2024-02-27T17:20:01.018+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: '/credentials.json'; 1531)
[2024-02-27T17:20:01.026+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:20:01.033+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:46:38.305+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:46:38.308+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:46:38.308+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:46:38.313+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:46:38.316+0000] {standard_task_runner.py:60} INFO - Started process 1969 to run task
[2024-02-27T17:46:38.319+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpk54aas6s']
[2024-02-27T17:46:38.320+0000] {standard_task_runner.py:88} INFO - Job 20: Subtask extract_task
[2024-02-27T17:46:38.340+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:46:38.364+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:46:38.364+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T17:46:38.367+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T174638, end_date=20240227T174638
[2024-02-27T17:46:38.371+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 20 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 1969)
[2024-02-27T17:46:38.418+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:46:38.424+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:51:30.002+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:51:30.005+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T17:51:30.005+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:51:30.010+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T17:51:30.013+0000] {standard_task_runner.py:60} INFO - Started process 2253 to run task
[2024-02-27T17:51:30.015+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpxsly8wy5']
[2024-02-27T17:51:30.017+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T17:51:30.033+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:51:30.056+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T17:51:30.057+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'dags/credentials.json'
[2024-02-27T17:51:30.059+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T175130, end_date=20240227T175130
[2024-02-27T17:51:30.063+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: 'dags/credentials.json'; 2253)
[2024-02-27T17:51:30.072+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:51:30.078+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:09:30.112+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:09:30.115+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:09:30.115+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:09:30.120+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:09:30.125+0000] {standard_task_runner.py:60} INFO - Started process 267 to run task
[2024-02-27T18:09:30.128+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpojcg50xm']
[2024-02-27T18:09:30.130+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:09:30.152+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host cf5ca63d0f74
[2024-02-27T18:09:30.176+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:09:30.177+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:09:30.179+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T180930, end_date=20240227T180930
[2024-02-27T18:09:30.183+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 267)
[2024-02-27T18:09:30.189+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:09:30.194+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:19:30.421+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:19:30.424+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:19:30.424+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:19:30.429+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:19:30.431+0000] {standard_task_runner.py:60} INFO - Started process 265 to run task
[2024-02-27T18:19:30.434+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp5z8gjb88']
[2024-02-27T18:19:30.435+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:19:30.452+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 38ece35a2749
[2024-02-27T18:19:30.476+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:19:30.476+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(credentials_path))
  File "/usr/local/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/usr/local/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/local/lib/python3.8/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
[2024-02-27T18:19:30.479+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T181930, end_date=20240227T181930
[2024-02-27T18:19:30.483+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task (Expecting value: line 1 column 1 (char 0); 265)
[2024-02-27T18:19:30.489+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:19:30.495+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:23:00.162+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:23:00.165+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:23:00.165+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:23:00.170+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:23:00.173+0000] {standard_task_runner.py:60} INFO - Started process 265 to run task
[2024-02-27T18:23:00.176+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpv587h3y7']
[2024-02-27T18:23:00.177+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:23:00.196+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:23:00.220+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:23:00.221+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(os.getenv('CREDENTIALS_KEY')))
  File "/usr/local/lib/python3.8/json/__init__.py", line 341, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not NoneType
[2024-02-27T18:23:00.223+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T182300, end_date=20240227T182300
[2024-02-27T18:23:00.227+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task (the JSON object must be str, bytes or bytearray, not NoneType; 265)
[2024-02-27T18:23:00.276+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:23:00.282+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:38:17.807+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:38:17.811+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:38:17.811+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:38:17.816+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:38:17.822+0000] {standard_task_runner.py:60} INFO - Started process 667 to run task
[2024-02-27T18:38:17.825+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpwlgla62e']
[2024-02-27T18:38:17.827+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:38:17.848+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:38:17.873+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:38:17.873+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:38:17.876+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T183817, end_date=20240227T183817
[2024-02-27T18:38:17.880+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 667)
[2024-02-27T18:38:17.923+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:38:17.930+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:39:26.269+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:39:26.272+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:39:26.272+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:39:26.277+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:39:26.281+0000] {standard_task_runner.py:60} INFO - Started process 972 to run task
[2024-02-27T18:39:26.284+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpafegijr5']
[2024-02-27T18:39:26.286+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:39:26.304+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:39:26.327+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:39:26.328+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:39:26.330+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T183926, end_date=20240227T183926
[2024-02-27T18:39:26.333+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 972)
[2024-02-27T18:39:26.343+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:39:26.349+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:46:44.220+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:46:44.222+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:46:44.222+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:46:44.228+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:46:44.232+0000] {standard_task_runner.py:60} INFO - Started process 275 to run task
[2024-02-27T18:46:44.235+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpk58vkb_n']
[2024-02-27T18:46:44.236+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:46:44.257+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:46:44.281+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:46:44.281+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:46:44.284+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T184644, end_date=20240227T184644
[2024-02-27T18:46:44.289+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 275)
[2024-02-27T18:46:44.294+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:46:44.299+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:49:50.840+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:49:50.843+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:49:50.843+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:49:50.848+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:49:50.852+0000] {standard_task_runner.py:60} INFO - Started process 823 to run task
[2024-02-27T18:49:50.855+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpxea8bk7o']
[2024-02-27T18:49:50.859+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:49:50.907+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:49:50.932+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:49:50.933+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/airflow/airflow/dags/credentials.json'
[2024-02-27T18:49:50.935+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T184950, end_date=20240227T184950
[2024-02-27T18:49:50.939+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: '/home/airflow/airflow/dags/credentials.json'; 823)
[2024-02-27T18:49:50.954+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:49:50.960+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:50:58.137+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:50:58.140+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:50:58.140+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:50:58.146+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:50:58.151+0000] {standard_task_runner.py:60} INFO - Started process 1123 to run task
[2024-02-27T18:50:58.154+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmplf3gdadp']
[2024-02-27T18:50:58.156+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:50:58.175+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:50:58.200+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:50:58.201+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: '/home/airflow/dags/credentials.json'
[2024-02-27T18:50:58.203+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T185058, end_date=20240227T185058
[2024-02-27T18:50:58.207+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task ([Errno 2] No such file or directory: '/home/airflow/dags/credentials.json'; 1123)
[2024-02-27T18:50:58.252+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:50:58.259+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:58:32.883+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:58:32.885+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [queued]>
[2024-02-27T18:58:32.885+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:58:32.891+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 05:00:00+00:00
[2024-02-27T18:58:32.895+0000] {standard_task_runner.py:60} INFO - Started process 412 to run task
[2024-02-27T18:58:32.897+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T05:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpqxw0sryx']
[2024-02-27T18:58:32.899+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask extract_task
[2024-02-27T18:58:32.918+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T05:00:00+00:00 [running]> on host 37df473fe932
[2024-02-27T18:58:32.942+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T05:00:00+00:00'
[2024-02-27T18:58:33.117+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:33.118+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:33.119+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:33.119+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T18:58:33.120+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 45, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard',
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T18:58:33.123+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T050000, start_date=20240227T185832, end_date=20240227T185833
[2024-02-27T18:58:33.129+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 17 for task extract_task (could not locate runnable browser; 412)
[2024-02-27T18:58:33.172+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:58:33.179+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
