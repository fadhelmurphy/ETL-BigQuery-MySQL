[2024-02-27T13:48:04.348+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T13:48:04.351+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T13:48:04.351+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T13:48:04.357+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T13:48:04.360+0000] {standard_task_runner.py:60} INFO - Started process 561 to run task
[2024-02-27T13:48:04.362+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/bbc_news_etl.py', '--cfg-path', '/tmp/tmp__mt8hxq']
[2024-02-27T13:48:04.364+0000] {standard_task_runner.py:88} INFO - Job 27: Subtask extract_task
[2024-02-27T13:48:04.381+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 169dadc36ff0
[2024-02-27T13:48:04.403+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T13:48:04.549+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:04.549+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:04.550+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T13:48:04.550+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T13:48:04.551+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/bbc_news_etl.py", line 44, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id='locator-173007', dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T13:48:04.554+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T134804, end_date=20240227T134804
[2024-02-27T13:48:04.559+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 27 for task extract_task (could not locate runnable browser; 561)
[2024-02-27T13:48:04.586+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T13:48:04.594+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T14:53:07.073+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T14:53:07.076+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T14:53:07.076+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T14:53:07.081+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T14:53:07.084+0000] {standard_task_runner.py:60} INFO - Started process 199 to run task
[2024-02-27T14:53:07.085+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/tsunami.py', '--cfg-path', '/tmp/tmppwmwp2ub']
[2024-02-27T14:53:07.087+0000] {standard_task_runner.py:88} INFO - Job 11: Subtask extract_task
[2024-02-27T14:53:07.103+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host b5306f90dbdb
[2024-02-27T14:53:07.124+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T14:53:07.255+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:07.256+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:07.257+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T14:53:07.257+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T14:53:07.258+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami.py", line 42, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard')
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T14:53:07.260+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T145307, end_date=20240227T145307
[2024-02-27T14:53:07.264+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 11 for task extract_task (could not locate runnable browser; 199)
[2024-02-27T14:53:07.313+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T14:53:07.322+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:00:06.048+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:00:06.051+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:00:06.051+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:00:06.056+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:00:06.060+0000] {standard_task_runner.py:60} INFO - Started process 334 to run task
[2024-02-27T17:00:06.063+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmphnnva7m9']
[2024-02-27T17:00:06.064+0000] {standard_task_runner.py:88} INFO - Job 11: Subtask extract_task
[2024-02-27T17:00:06.082+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:00:06.130+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:00:06.132+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 46, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:00:06.136+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T170006, end_date=20240227T170006
[2024-02-27T17:00:06.140+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 11 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 334)
[2024-02-27T17:00:06.158+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:00:06.167+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:04:24.050+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:04:24.053+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:04:24.054+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:04:24.061+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:04:24.069+0000] {standard_task_runner.py:60} INFO - Started process 545 to run task
[2024-02-27T17:04:24.074+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp7gwo2esj']
[2024-02-27T17:04:24.076+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T17:04:24.094+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:04:24.117+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:04:24.118+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/{CREDENTIALS_FILE}")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/None'
[2024-02-27T17:04:24.121+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T170424, end_date=20240227T170424
[2024-02-27T17:04:24.125+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: '../auth/json/None'; 545)
[2024-02-27T17:04:24.133+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:04:24.141+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:06:28.428+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:06:28.431+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:06:28.431+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:06:28.437+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:06:28.441+0000] {standard_task_runner.py:60} INFO - Started process 737 to run task
[2024-02-27T17:06:28.445+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpzdg45jf6']
[2024-02-27T17:06:28.447+0000] {standard_task_runner.py:88} INFO - Job 42: Subtask extract_task
[2024-02-27T17:06:28.466+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:06:28.489+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:06:28.490+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(f"../auth/json/credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '../auth/json/credentials.json'
[2024-02-27T17:06:28.493+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T170628, end_date=20240227T170628
[2024-02-27T17:06:28.497+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 42 for task extract_task ([Errno 2] No such file or directory: '../auth/json/credentials.json'; 737)
[2024-02-27T17:06:28.501+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:06:28.509+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:09:53.861+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:09:53.863+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:09:53.864+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:09:53.870+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:09:53.873+0000] {standard_task_runner.py:60} INFO - Started process 936 to run task
[2024-02-27T17:09:53.876+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmptstenzw4']
[2024-02-27T17:09:53.877+0000] {standard_task_runner.py:88} INFO - Job 10: Subtask extract_task
[2024-02-27T17:09:53.896+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:09:53.928+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:09:53.929+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:09:53.932+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T170953, end_date=20240227T170953
[2024-02-27T17:09:53.937+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 10 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 936)
[2024-02-27T17:09:53.975+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:09:53.985+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:16:26.441+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:16:26.444+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:16:26.444+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:16:26.449+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:16:26.461+0000] {standard_task_runner.py:60} INFO - Started process 1315 to run task
[2024-02-27T17:16:26.463+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpwv3rgxwv']
[2024-02-27T17:16:26.465+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T17:16:26.492+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:16:26.528+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:16:26.529+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 41, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file("credentials.json")
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'credentials.json'
[2024-02-27T17:16:26.532+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T171626, end_date=20240227T171626
[2024-02-27T17:16:26.536+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: 'credentials.json'; 1315)
[2024-02-27T17:16:26.561+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:16:26.567+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:20:17.795+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:20:17.798+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:20:17.798+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:20:17.803+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:20:17.807+0000] {standard_task_runner.py:60} INFO - Started process 1630 to run task
[2024-02-27T17:20:17.810+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpnf4y51m9']
[2024-02-27T17:20:17.812+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T17:20:17.833+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:20:17.858+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:20:17.859+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/credentials.json'
[2024-02-27T17:20:17.861+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T172017, end_date=20240227T172017
[2024-02-27T17:20:17.865+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: '/credentials.json'; 1630)
[2024-02-27T17:20:17.870+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:20:17.876+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T17:51:43.216+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:51:43.219+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T17:51:43.219+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T17:51:43.224+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T17:51:43.227+0000] {standard_task_runner.py:60} INFO - Started process 2352 to run task
[2024-02-27T17:51:43.230+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpgmo9juox']
[2024-02-27T17:51:43.231+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T17:51:43.250+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 45e004960ef9
[2024-02-27T17:51:43.273+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T17:51:43.274+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: 'dags/credentials.json'
[2024-02-27T17:51:43.276+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T175143, end_date=20240227T175143
[2024-02-27T17:51:43.281+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: 'dags/credentials.json'; 2352)
[2024-02-27T17:51:43.287+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T17:51:43.292+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:09:43.842+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:09:43.844+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:09:43.844+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:09:43.850+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:09:43.853+0000] {standard_task_runner.py:60} INFO - Started process 368 to run task
[2024-02-27T18:09:43.855+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpl9lq2ux4']
[2024-02-27T18:09:43.857+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:09:43.874+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host cf5ca63d0f74
[2024-02-27T18:09:43.896+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:09:43.897+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 42, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_file(credentials_path)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/oauth2/service_account.py", line 257, in from_service_account_file
    info, signer = _service_account_info.from_filename(
  File "/home/airflow/.local/lib/python3.8/site-packages/google/auth/_service_account_info.py", line 78, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:09:43.899+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T180943, end_date=20240227T180943
[2024-02-27T18:09:43.903+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 368)
[2024-02-27T18:09:43.913+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:09:43.919+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:19:44.060+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:19:44.062+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:19:44.062+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:19:44.067+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:19:44.070+0000] {standard_task_runner.py:60} INFO - Started process 364 to run task
[2024-02-27T18:19:44.071+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpbcuz0yxp']
[2024-02-27T18:19:44.073+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:19:44.089+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 38ece35a2749
[2024-02-27T18:19:44.110+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:19:44.111+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(credentials_path))
  File "/usr/local/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/usr/local/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/local/lib/python3.8/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
[2024-02-27T18:19:44.113+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T181944, end_date=20240227T181944
[2024-02-27T18:19:44.117+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task (Expecting value: line 1 column 1 (char 0); 364)
[2024-02-27T18:19:44.128+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:19:44.134+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:23:14.150+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:23:14.152+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:23:14.152+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:23:14.158+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:23:14.162+0000] {standard_task_runner.py:60} INFO - Started process 366 to run task
[2024-02-27T18:23:14.165+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpkodgg4di']
[2024-02-27T18:23:14.167+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:23:14.194+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:23:14.245+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:23:14.246+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    GCPCredentials = service_account.Credentials.from_service_account_info(json.loads(os.getenv('CREDENTIALS_KEY')))
  File "/usr/local/lib/python3.8/json/__init__.py", line 341, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
TypeError: the JSON object must be str, bytes or bytearray, not NoneType
[2024-02-27T18:23:14.249+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T182314, end_date=20240227T182314
[2024-02-27T18:23:14.254+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task (the JSON object must be str, bytes or bytearray, not NoneType; 366)
[2024-02-27T18:23:14.263+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:23:14.274+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:38:34.019+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:38:34.022+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:38:34.022+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:38:34.027+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:38:34.030+0000] {standard_task_runner.py:60} INFO - Started process 768 to run task
[2024-02-27T18:38:34.032+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp24tz9nqu']
[2024-02-27T18:38:34.033+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:38:34.049+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:38:34.071+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:38:34.071+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:38:34.073+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T183834, end_date=20240227T183834
[2024-02-27T18:38:34.076+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 768)
[2024-02-27T18:38:34.089+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:38:34.095+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:39:42.017+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:39:42.020+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:39:42.020+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:39:42.026+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:39:42.031+0000] {standard_task_runner.py:60} INFO - Started process 1071 to run task
[2024-02-27T18:39:42.034+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpzvdhm9yi']
[2024-02-27T18:39:42.036+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:39:42.067+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host f8b0167ca209
[2024-02-27T18:39:42.113+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:39:42.114+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:39:42.118+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T183942, end_date=20240227T183942
[2024-02-27T18:39:42.123+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 1071)
[2024-02-27T18:39:42.182+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:39:42.199+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:47:00.218+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:47:00.220+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:47:00.221+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:47:00.226+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:47:00.229+0000] {standard_task_runner.py:60} INFO - Started process 374 to run task
[2024-02-27T18:47:00.231+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmpe3gzfc0k']
[2024-02-27T18:47:00.232+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:47:00.249+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host c200f435f34d
[2024-02-27T18:47:00.270+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:47:00.271+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 44, in extract_tsunami
    with open(credentials_path, "r") as file:
FileNotFoundError: [Errno 2] No such file or directory: './credentials.json'
[2024-02-27T18:47:00.273+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T184700, end_date=20240227T184700
[2024-02-27T18:47:00.276+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task ([Errno 2] No such file or directory: './credentials.json'; 374)
[2024-02-27T18:47:00.288+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:47:00.294+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-27T18:58:54.708+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:58:54.710+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [queued]>
[2024-02-27T18:58:54.711+0000] {taskinstance.py:2171} INFO - Starting attempt 1 of 2
[2024-02-27T18:58:54.718+0000] {taskinstance.py:2192} INFO - Executing <Task(PythonOperator): extract_task> on 2024-01-01 08:00:00+00:00
[2024-02-27T18:58:54.725+0000] {standard_task_runner.py:60} INFO - Started process 513 to run task
[2024-02-27T18:58:54.731+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'tsunami_etl', 'extract_task', 'scheduled__2024-01-01T08:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/tsunami_etl.py', '--cfg-path', '/tmp/tmp0ve7bim9']
[2024-02-27T18:58:54.733+0000] {standard_task_runner.py:88} INFO - Job 26: Subtask extract_task
[2024-02-27T18:58:54.754+0000] {task_command.py:423} INFO - Running <TaskInstance: tsunami_etl.extract_task scheduled__2024-01-01T08:00:00+00:00 [running]> on host 37df473fe932
[2024-02-27T18:58:54.778+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tsunami_etl' AIRFLOW_CTX_TASK_ID='extract_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T08:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T08:00:00+00:00'
[2024-02-27T18:58:54.941+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:54.942+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:54.943+0000] {_metadata.py:139} WARNING - Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
[2024-02-27T18:58:54.943+0000] {_default.py:338} WARNING - Authentication failed using Compute Engine authentication due to unavailable metadata server.
[2024-02-27T18:58:54.945+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/tsunami_etl.py", line 45, in extract_tsunami
    return pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard',
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 934, in read_gbq
    connector = GbqConnector(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/gbq.py", line 318, in __init__
    self.credentials, default_project = auth.get_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas_gbq/auth.py", line 56, in get_credentials
    credentials, default_project_id = pydata_google_auth.default(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 152, in default
    credentials = get_user_credentials(
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/auth.py", line 362, in get_user_credentials
    credentials = _webserver.run_local_server(app_flow, **AUTH_URI_KWARGS)
  File "/home/airflow/.local/lib/python3.8/site-packages/pydata_google_auth/_webserver.py", line 89, in run_local_server
    return app_flow.run_local_server(host=LOCALHOST, port=port, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google_auth_oauthlib/flow.py", line 444, in run_local_server
    webbrowser.get(browser).open(auth_url, new=1, autoraise=True)
  File "/usr/local/lib/python3.8/webbrowser.py", line 65, in get
    raise Error("could not locate runnable browser")
webbrowser.Error: could not locate runnable browser
[2024-02-27T18:58:54.953+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=tsunami_etl, task_id=extract_task, execution_date=20240101T080000, start_date=20240227T185854, end_date=20240227T185854
[2024-02-27T18:58:54.962+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 26 for task extract_task (could not locate runnable browser; 513)
[2024-02-27T18:58:54.992+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T18:58:54.998+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
